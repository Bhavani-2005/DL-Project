{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1VvHreTKfyxvTgoi_xPcs_wTWw4YsWZ5g",
      "authorship_tag": "ABX9TyOf7DAmPkb/in5lm/Gx0IHR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhavani-2005/DL-Project/blob/main/Image_caption_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "oQiH2bGtq2EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import json\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n"
      ],
      "metadata": {
        "id": "0eNfXNJ0q_sI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataSet"
      ],
      "metadata": {
        "id": "kpUpUhlxwxV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aMRbTPgwzto",
        "outputId": "5df81ee3-6193-4baf-f49b-630b05be4bf4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive\"\n"
      ],
      "metadata": {
        "id": "L2x3TEzhx162",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "340ab84e-021b-4102-a7dd-5665f8437990"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Colab Notebooks'  'Image Caption generation'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/Image Caption generation/DataSet'\n",
        "\n",
        "# List the contents of dataset folder\n",
        "print(\"Dataset folder contents:\", os.listdir(dataset_path))\n",
        "\n",
        "# Set correct paths\n",
        "image_folder = os.path.join(dataset_path, 'Images')\n",
        "caption_file = os.path.join(dataset_path, 'captions.txt')\n",
        "\n",
        "# Verify the files exist\n",
        "print(\"Images folder contains:\", len(os.listdir(image_folder)), \"files\")\n",
        "print(\"First 5 images:\", os.listdir(image_folder)[:5])\n",
        "print(\"Captions file exists:\", os.path.isfile(caption_file))\n"
      ],
      "metadata": {
        "id": "Qz6vhGMTx518",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ee71aa-b374-4028-f90e-6358d41490a5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset folder contents: ['Images', 'captions.txt']\n",
            "Images folder contains: 2372 files\n",
            "First 5 images: ['1457762320_7fe121b285.jpg', '1466479163_439db855af.jpg', '146098876_0d99d7fb98.jpg', '1468429623_f001988691.jpg', '1448511770_1a4a9c453b.jpg']\n",
            "Captions file exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing Captions"
      ],
      "metadata": {
        "id": "VFAEFj-_14Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# Load captions file\n",
        "with open(caption_file, 'r') as f:\n",
        "    captions_doc = f.read()\n",
        "\n",
        "# Create dictionary mapping image names to their captions\n",
        "captions_dict = {}\n",
        "\n",
        "for line in captions_doc.strip().split('\\n'):\n",
        "    if len(line) < 1:\n",
        "        continue\n",
        "    img_cap_pair = line.split('\\t')\n",
        "    if len(img_cap_pair) == 1:\n",
        "        img_cap_pair = line.split(' ')\n",
        "        img_name = img_cap_pair[0].split('#')[0]\n",
        "        caption = ' '.join(img_cap_pair[1:])\n",
        "    else:\n",
        "        img_name, caption = img_cap_pair\n",
        "    img_name = img_name.split('#')[0]\n",
        "    caption = caption.lower()\n",
        "    caption = caption.translate(str.maketrans('', '', string.punctuation))\n",
        "    caption = '<start> ' + caption + ' <end>'\n",
        "    if img_name not in captions_dict:\n",
        "        captions_dict[img_name] = []\n",
        "    captions_dict[img_name].append(caption)\n",
        "\n",
        "# Show example output\n",
        "for img, caps in list(captions_dict.items())[:1]:\n",
        "    print(\"Image:\", img)\n",
        "    print(\"Captions:\")\n",
        "    for c in caps:\n",
        "        print(\"-\", c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6g07CT3163W",
        "outputId": "fc72e933-a36c-4ef0-8a6c-b9930e76dabc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: image,caption\n",
            "Captions:\n",
            "- <start>  <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing Captions"
      ],
      "metadata": {
        "id": "2eOf0lti3MrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import pickle\n",
        "\n",
        "# Gather all captions into a list\n",
        "all_captions = []\n",
        "for key in captions_dict:\n",
        "    all_captions.extend(captions_dict[key])\n",
        "\n",
        "# Initialize the tokenizer and fit on captions\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "\n",
        "# Vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary size:\", vocab_size)\n",
        "\n",
        "# Save the tokenizer for later use\n",
        "tokenizer_path = '/content/drive/MyDrive/Image Caption generation/DataSet/tokenizer.pkl'\n",
        "with open(tokenizer_path, 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "print(f\"Tokenizer saved at: {tokenizer_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbxeXMDj3Q6W",
        "outputId": "2079344f-1ec4-41c3-efa6-e4a7d69b1361"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 8746\n",
            "Tokenizer saved at: /content/drive/MyDrive/Image Caption generation/DataSet/tokenizer.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find Maximum Caption Length"
      ],
      "metadata": {
        "id": "CQ_hPGf433SR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the maximum length among all captions\n",
        "max_length = max(len(c.split()) for c in all_captions)\n",
        "\n",
        "print(\"Maximum caption length:\", max_length)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s5-61rJ4YmW",
        "outputId": "0fd4364a-5901-4604-eb71-3fd624c786d9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum caption length: 37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting Image Features"
      ],
      "metadata": {
        "id": "MA1DaOxQ4fso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Load the InceptionV3 model without the final classification layer\n",
        "base_model = InceptionV3(weights='imagenet')\n",
        "model = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
        "\n",
        "# Path to images folder\n",
        "image_folder = '/content/drive/MyDrive/Image Caption generation/DataSet/Images'\n",
        "\n",
        "# Dictionary to hold image features\n",
        "features = {}\n",
        "\n",
        "# Process and extract features for each image\n",
        "for img_name in os.listdir(image_folder):\n",
        "    img_path = os.path.join(image_folder, img_name)\n",
        "    # Load the image with target size expected by InceptionV3\n",
        "    image = load_img(img_path, target_size=(299, 299))\n",
        "    image = img_to_array(image)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = preprocess_input(image)\n",
        "    # Extract features\n",
        "    feature = model.predict(image, verbose=0)\n",
        "    features[img_name] = feature\n",
        "\n",
        "# Save the features to a file\n",
        "features_path = '/content/drive/MyDrive/Image Caption generation/DataSet/features.pkl'\n",
        "with open(features_path, 'wb') as f:\n",
        "    pickle.dump(features, f)\n",
        "\n",
        "print(f\"Extracted features for {len(features)} images and saved to {features_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msSKY-SG4vxO",
        "outputId": "27c03bcc-cc17-4bce-f810-caf29220c8f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "\u001b[1m96112376/96112376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    }
  ]
}